{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7200ca9c-c0c1-47e0-84d8-4f37c1680072",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10faf651-c571-435f-aeba-b997db905764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "462a0567-701a-4b7f-91ae-36110acdba57",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LossLoggingCallback' from 'pytorch_lightning.callbacks' (C:\\Users\\uzair\\.conda\\envs\\TCN\\lib\\site-packages\\pytorch_lightning\\callbacks\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, LossLoggingCallback\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdarts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstatistics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_seasonality\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdarts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mse\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LossLoggingCallback' from 'pytorch_lightning.callbacks' (C:\\Users\\uzair\\.conda\\envs\\TCN\\lib\\site-packages\\pytorch_lightning\\callbacks\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import random\n",
    "from darts import TimeSeries\n",
    "from darts.metrics import mae, rmse\n",
    "from darts.ad import ForecastingAnomalyModel, KMeansScorer, NormScorer\n",
    "from darts.ad.detectors import QuantileDetector\n",
    "from darts.models import TCNModel\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LossLoggingCallback\n",
    "from darts.utils.statistics import check_seasonality\n",
    "from darts.metrics import mse\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7417207f-c5b4-41c0-aef0-f8f3c770b059",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a650f1-af98-4bc0-8e97-46af1e62f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"D:/internship project/ECG5000_TRAIN.txt\", delimiter='\\s+', header=None)\n",
    "df_2 = pd.read_csv(\"D:/internship project/ECG5000_TEST.txt\", delimiter='\\s+', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202336c1-d637-4a83-a661-50ef4cad72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_1, df_2], ignore_index=True)\n",
    "df.to_csv('D:/internship project/Combined_data.csv', index=False, header=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07ca523-7386-4c9d-b99a-884e511c9046",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9540b4-949a-4bca-aa94-6d31a79d2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6905c256-8f01-461d-b7d5-a6f47ca3f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1db10f-ec5b-463e-8570-32c891fa2f8f",
   "metadata": {},
   "source": [
    "# Extracting Normal, Abnormal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8d53be-9eda-4934-a905-88b955780ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = df.loc[df[0] == 1]\n",
    "abnormal_data = df.loc[df[0] != 1]\n",
    "normal_data.to_csv('D:/internship project/normal data.csv', index=False)\n",
    "abnormal_data.to_csv('D:/internship project/abnormal data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0994b646-13c3-4cab-bd09-c77497d54d10",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dd877b-dbc4-4143-a213-ad655bd22b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_signals = df.iloc[:, 1:]\n",
    "\n",
    "plt.figure(figsize=(20, 30))\n",
    "for index, row in ecg_signals.iterrows():\n",
    "    plt.plot(row, label=f'Signal {index + 1}')\n",
    "plt.xlabel('Time (Sample Points)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('ECG Signals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a075c7c3-3078-47df-98a1-cafec117312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_signals = normal_data.iloc[:, 1:]\n",
    "\n",
    "plt.figure(figsize=(20, 30))\n",
    "for index, row in ecg_signals.iterrows():\n",
    "    plt.plot(row, label=f'Signal {index + 1}')\n",
    "plt.xlabel('Time (Sample Points)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Normal ECG Signals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8d24b9-f69c-48a2-9ba6-98b88d4cb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming normal_data contains the normal ECG signals\n",
    "normal_signals = normal_data.iloc[:, 1:]  # Adjust the slicing as per your data structure\n",
    "selected_normal_signal = normal_signals.iloc[random.randint(0, len(normal_signals) - 1)]\n",
    "\n",
    "# Assuming abnormal_data contains the abnormal ECG signals\n",
    "abnormal_signals = abnormal_data.iloc[:, 1:]  # Adjust the slicing as per your data structure\n",
    "selected_abnormal_signal = abnormal_signals.iloc[random.randint(0, len(abnormal_signals) - 1)]\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot the normal ECG signal\n",
    "axs[0].plot(selected_normal_signal, label='Normal ECG Signal', color='blue')\n",
    "axs[0].set_xlabel('Time (Sample Points)')\n",
    "axs[0].set_ylabel('Amplitude')\n",
    "axs[0].set_title('Normal ECG Signal')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the abnormal ECG signal\n",
    "axs[1].plot(selected_abnormal_signal, label='Abnormal ECG Signal', color='red')\n",
    "axs[1].set_xlabel('Time (Sample Points)')\n",
    "axs[1].set_ylabel('Amplitude')\n",
    "axs[1].set_title('Abnormal ECG Signal')\n",
    "axs[1].legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7be316c-58bd-4464-ac45-2566e06a3f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming ecg_signals contains the normal data\n",
    "normal_signals = normal_data.iloc[:, 1:]  # Adjust the slicing as per your data structure\n",
    "selected_normal_signal = normal_signals.iloc[random.randint(0, len(normal_signals) - 1)]\n",
    "\n",
    "# Assuming ecg_signals contains the abnormal data\n",
    "abnormal_signals = abnormal_data.iloc[:, 1:]  # Adjust the slicing as per your data structure\n",
    "selected_abnormal_signal = abnormal_signals.iloc[random.randint(0, len(abnormal_signals) - 1)]\n",
    "\n",
    "# Plotting both normal and abnormal signals on the same plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(selected_normal_signal, label='Normal ECG Signal', color='blue')\n",
    "plt.plot(selected_abnormal_signal, label='Abnormal ECG Signal', color='red')\n",
    "plt.xlabel('Time (Sample Points)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Comparison of Normal and Abnormal ECG Signals')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f7da9-9fc5-481c-9d4b-20c7d9c9ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(normal_data, bins=75, alpha=0.7, label='Normal ECG Signal')\n",
    "plt.hist(abnormal_data, bins=75, alpha=0.7, label='Abnormal ECG Signal')\n",
    "plt.xlabel('Amplitude')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of ECG Signal Amplitudes')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13633b4-7be3-42cf-ae9d-037d7ee71863",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot density for normal ECG signals without labels in the legend\n",
    "sns.kdeplot(normal_data, fill=True, alpha=0.5, color='blue', legend=False)\n",
    "\n",
    "# Plot density for abnormal ECG signals without labels in the legend\n",
    "sns.kdeplot(abnormal_data, fill=True, alpha=0.5, color='red', legend=False)\n",
    "\n",
    "plt.xlabel('Amplitude')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of ECG Signal Amplitudes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f611225-aa15-40b4-a6af-1947b832f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all columns of normal and abnormal data into single series\n",
    "normal_combined = normal_data.values.flatten()\n",
    "abnormal_combined = abnormal_data.values.flatten()\n",
    "\n",
    "# Plotting the density plots\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.kdeplot(normal_combined, label='Normal ECG Signal', fill=True, alpha=0.5, color='blue')\n",
    "sns.kdeplot(abnormal_combined, label='Abnormal ECG Signal', fill=True, alpha=0.5, color='red')\n",
    "plt.xlabel('Amplitude')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of ECG Signal Amplitudes')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64075aa6-79df-4e72-bb51-090551345ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(normal_data.iloc[:10, :], cmap='viridis')\n",
    "plt.xlabel('Time (Sample Points)')\n",
    "plt.ylabel('ECG Signal Index')\n",
    "plt.title('Heatmap of Normal ECG Signals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb944d-63b6-4e7a-b0a2-de26f00f9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for plotting\n",
    "boxplot_data = pd.DataFrame({\n",
    "    'Amplitude': pd.concat([selected_normal_signal, selected_abnormal_signal]),\n",
    "    'Type': ['Normal'] * len(selected_normal_signal) + ['Abnormal'] * len(selected_abnormal_signal)\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# Basic Boxplot\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(x='Type', y='Amplitude', data=boxplot_data, palette='Set2')\n",
    "plt.title('Basic Boxplot of ECG Signal Amplitudes')\n",
    "\n",
    "# Detailed Boxplot\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='Type', y='Amplitude', data=boxplot_data, palette='Set2', showfliers=False)\n",
    "sns.swarmplot(x='Type', y='Amplitude', data=boxplot_data, color='k', alpha=0.5, dodge=True)\n",
    "\n",
    "# Add mean markers\n",
    "mean_values = boxplot_data.groupby('Type')['Amplitude'].mean()\n",
    "for i, mean in enumerate(mean_values):\n",
    "    plt.scatter(x=i, y=mean, color='red', marker='D', s=100, label='Mean' if i == 0 else \"\", zorder=10)\n",
    "\n",
    "plt.title('Detailed Boxplot of ECG Signal Amplitudes')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e302df33-bc29-49a6-b410-48c59689e61a",
   "metadata": {},
   "source": [
    "# Remove Label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0fddb1-603a-4d90-8788-219da2a71e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data.drop(normal_data.columns[0], axis=1, inplace=True)\n",
    "abnormal_data.drop(abnormal_data.columns[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21272026-00d5-469f-ba0a-982a7f1a5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427d6547-8e7d-4c1d-b046-e893adca0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e958710-a0db-422b-a91b-fd01f418b523",
   "metadata": {},
   "source": [
    "# Train, Validation and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db317fd-ceab-4f68-8529-c56d68cb96b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and remaining (test + validation)\n",
    "train_data, temp_data = train_test_split(normal_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the remaining data into test and validation\n",
    "test_data, val_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Print the shapes of the datasets\n",
    "print(f\"Training Data Shape: {train_data.shape}\")\n",
    "print(f\"Validation Data Shape: {val_data.shape}\")\n",
    "print(f\"Test Data Shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54ce9f5-44e9-497c-b197-7414df2f66d5",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf8da0-952a-4354-8078-8d7279ad7b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit on training data and transform train, validation, and test sets\n",
    "train_features_scaled = scaler.fit_transform(train_data)\n",
    "val_features_scaled = scaler.transform(val_data)\n",
    "test_features_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Convert scaled features back to DataFrames if needed\n",
    "train_data_scaled = pd.DataFrame(train_features_scaled, columns=train_data.columns)\n",
    "val_data_scaled = pd.DataFrame(val_features_scaled, columns=val_data.columns)\n",
    "test_data_scaled = pd.DataFrame(test_features_scaled, columns=test_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e9324-607d-4638-b7ce-7e6e360fbd61",
   "metadata": {},
   "source": [
    "# Remove Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e885a0-9a8d-44c2-8a20-f59c334d16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove index\n",
    "\n",
    "train_data_scaled.reset_index(drop=True, inplace=True)\n",
    "val_data_scaled.reset_index(drop=True, inplace=True)\n",
    "test_data_scaled.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa527c9-7b83-4bb8-b568-a9135b1a6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954bf36-2b4b-44bb-b57c-8fb588c74f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6889bb-d7df-4b28-95a7-e8c20b0f70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f558554-7701-4da7-ab95-77b038b155ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Printing infos\n",
    "train_data_scaled.info()\n",
    "val_data_scaled.info()\n",
    "test_data_scaled.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015a7ba7-38ed-4053-b030-459aa180bab4",
   "metadata": {},
   "source": [
    "# Adding TimeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b303350a-1560-4e82-afab-d4067fd8c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series = TimeSeries.from_dataframe(train_data_scaled)\n",
    "val_series = TimeSeries.from_dataframe(val_data_scaled)\n",
    "test_series = TimeSeries.from_dataframe(test_data_scaled)\n",
    "train_series = train_series.astype(np.float32)\n",
    "val_series = val_series.astype(np.float32)\n",
    "test_series = test_series.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d2848-c709-4893-acec-ce4db5d6be29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_series.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b1f24a-5fc1-4d6e-98e7-efa06913a3d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_series.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07df8405-169f-4524-a4c9-2e52e03370f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_series.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eb8988-ac98-44b1-bae2-15e850b7b3a9",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30909ec6-1f6a-47c1-bc8b-92fffacb65e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLoggingCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.metrics = {\"epochs\": [], \"train_loss\": [], \"val_loss\": []}\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        train_loss = trainer.callback_metrics.get(\"train_loss\", None)\n",
    "        if train_loss is not None:\n",
    "            train_loss = train_loss.item()\n",
    "            self.train_losses.append(train_loss)\n",
    "            print(f\"Train epoch end: recorded train loss {train_loss}\")\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        val_loss = trainer.callback_metrics.get(\"val_loss\", None)\n",
    "        print(\"Validation Epoch End Callback Triggered\")  # Debugging Line\n",
    "        if val_loss is not None:\n",
    "            val_loss = val_loss.item()\n",
    "            self.val_losses.append(val_loss)\n",
    "            print(f\"Validation epoch end: recorded validation loss {val_loss}\")\n",
    "\n",
    "            # Append new metrics\n",
    "            epoch = trainer.current_epoch\n",
    "            self.metrics[\"epochs\"].append(epoch)\n",
    "            self.metrics[\"train_loss\"].append(self.train_losses[-1] if self.train_losses else None)\n",
    "            self.metrics[\"val_loss\"].append(val_loss)\n",
    "\n",
    "\n",
    "from darts.models import TCNModel\n",
    "from darts.callbacks import EarlyStopping, LossLoggingCallback\n",
    "from darts.trainers import Trainer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define a more complex TCN model\n",
    "ecg_model = TCNModel(\n",
    "    input_chunk_length=50,\n",
    "    output_chunk_length=30,\n",
    "    kernel_size=5,                # Increased kernel size\n",
    "    num_filters=64,               # Increased number of filters\n",
    "    num_layers=5,                 # Increased number of layers\n",
    "    dropout=0.3,                  # Adjusted dropout rate\n",
    "    residual=True,                # Added residual connections\n",
    "    optimizer_cls=Adam,\n",
    "    optimizer_kwargs={\"lr\": 0.001},\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # Metric to monitor\n",
    "    patience=10,         # Number of epochs to wait for improvement\n",
    "    mode=\"min\"           # Mode should be 'min' for loss metrics\n",
    ")\n",
    "\n",
    "# Create the loss logging callback instance\n",
    "loss_callback = LossLoggingCallback()\n",
    "\n",
    "# Initialize the trainer with callbacks\n",
    "trainer = Trainer(\n",
    "    callbacks=[early_stopping_callback, loss_callback],\n",
    "    max_epochs=100,\n",
    "    logger=True,\n",
    "    enable_progress_bar=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d95f0f-05f6-43f4-b673-f64fcd12a649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ecg_model.fit(train_series, val_series=val_series, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0bd5c5-2af5-4428-bdb6-5bc101e93812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to have matching lengths for epochs, train_losses, and val_losses\n",
    "epochs = range(len(loss_callback.train_losses))\n",
    "\n",
    "# Ensure that the lengths match\n",
    "num_train_epochs = len(loss_callback.train_losses)\n",
    "num_val_epochs = len(loss_callback.val_losses)\n",
    "\n",
    "# Adjust val_losses if it has more entries than train_losses\n",
    "if num_val_epochs > num_train_epochs:\n",
    "    loss_callback.val_losses = loss_callback.val_losses[:num_train_epochs]\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, loss_callback.train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(epochs, loss_callback.val_losses, label='Validation Loss', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df89da-4d5a-4abc-9332-fec10f7b4c2a",
   "metadata": {},
   "source": [
    "# Forecasting Anomaly Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53045409-f73f-4ca1-83ed-a84f47bdbfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_features_scaled = scaler.transform(abnormal_data)\n",
    "abnormal_data_scaled = pd.DataFrame(abnormal_features_scaled, columns=test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45766a3-63b0-49b5-982e-9b248b05d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_series = TimeSeries.from_dataframe(abnormal_data_scaled)\n",
    "abnormal_series = abnormal_series.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29822d8c-0bd2-4d6e-b793-41d644286ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the anomaly model with: one fitted model, and 3 scorers\n",
    "anomaly_model = ForecastingAnomalyModel(\n",
    "    model=ecg_model,\n",
    "    scorer=[\n",
    "        NormScorer(ord=1),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b95f0-292b-47b5-b39c-5761bf7f71cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = 0.1\n",
    "anomaly_model.fit(train_series, start=START, allow_model_training=False, verbose=True, scorer=NormScorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49862e2b-2acf-41ec-9c49-29c24fb8b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate anomaly scores for the validation or test series\n",
    "anomaly_scores, model_forecasting = anomaly_model.score(\n",
    "    test_series, start=START, return_model_prediction=True, verbose=True\n",
    ")\n",
    "pred_start = model_forecasting.start_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ec3351-1c1b-4f88-b842-70d9eac19d6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract anomaly scores from the result\n",
    "anomaly_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a2b93c-79e9-4049-a7c5-98251063b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the time index and values from the TimeSeries object\n",
    "time_index = anomaly_scores.time_index\n",
    "scores = anomaly_scores.values()\n",
    "\n",
    "# Plot the anomaly scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time_index, scores, label=\"Anomaly Scores\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Anomaly Scores')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dade20-1af5-4477-b26f-04f055f35e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae(model_forecasting, test_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24933f6f-1371-4cc8-a283-b560f2b54e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(model_forecasting, test_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28218c12-02d1-436c-bc12-c974fc0e7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate anomaly scores on the validation data\n",
    "val_anomaly_scores, val_model_forecasting = anomaly_model.score(\n",
    "    val_series, start=START, return_model_prediction=True, verbose=True\n",
    ")\n",
    "pred_start = model_forecasting.start_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d068072-ca4d-4512-b5be-d9fdde3f6b1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_anomaly_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949874c2-d0b2-4413-8566-bc854cf03ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays if they are not already\n",
    "time_index = np.array(time_index)\n",
    "scores = np.array(scores)\n",
    "\n",
    "# Calculate z-scores\n",
    "mean_score = np.mean(scores)\n",
    "std_dev_score = np.std(scores)\n",
    "z_scores = (scores - mean_score) / std_dev_score\n",
    "\n",
    "# Define the threshold for anomaly detection\n",
    "threshold = 3\n",
    "anomalies = z_scores > threshold\n",
    "anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f314326f-0324-4a96-9639-3f1d66b57cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the types and shapes of the variables\n",
    "print(f\"Type of time_index: {type(time_index)}\")\n",
    "print(f\"Shape of time_index: {np.shape(time_index)}\")\n",
    "print(f\"Type of scores: {type(scores)}\")\n",
    "print(f\"Shape of scores: {np.shape(scores)}\")\n",
    "print(f\"Type of anomalies: {type(anomalies)}\")\n",
    "print(f\"Shape of anomalies: {np.shape(anomalies)}\")\n",
    "\n",
    "# Convert to numpy arrays if they are not already\n",
    "if not isinstance(time_index, np.ndarray):\n",
    "    time_index = np.array(time_index)\n",
    "if not isinstance(scores, np.ndarray):\n",
    "    scores = np.array(scores)\n",
    "if not isinstance(anomalies, np.ndarray):\n",
    "    anomalies = np.array(anomalies)\n",
    "\n",
    "# Verify the shapes after conversion\n",
    "print(f\"Converted type of time_index: {type(time_index)}\")\n",
    "print(f\"Shape of time_index after conversion: {np.shape(time_index)}\")\n",
    "print(f\"Converted type of scores: {type(scores)}\")\n",
    "print(f\"Shape of scores after conversion: {np.shape(scores)}\")\n",
    "print(f\"Converted type of anomalies: {type(anomalies)}\")\n",
    "print(f\"Shape of anomalies after conversion: {np.shape(anomalies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d28a7-37c9-48c3-b0a6-a6dc2a35bd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the scores and anomalies arrays to 1D\n",
    "scores = scores.flatten()\n",
    "anomalies = anomalies.flatten()\n",
    "\n",
    "# Plot the anomaly scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time_index, scores, label=\"Anomaly Scores\")\n",
    "plt.scatter(time_index[anomalies], scores[anomalies], color='red', label=\"Detected Anomalies\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Anomaly Scores with Detected Anomalies')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae44f785-397e-4512-890c-cc65aaf468a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the indices of detected anomalies\n",
    "print(\"Anomaly indices:\", np.where(anomalies)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60379c-984f-4d1b-9786-a9e37abb06ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chunk size\n",
    "chunk_size = 50  # Example chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a044d7-ef34-4013-b384-2043d9e53aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate anomaly scores for each chunk\n",
    "def calculate_anomaly_scores_by_chunk(time_index, scores, chunk_size, threshold=3):\n",
    "    num_chunks = len(time_index) // chunk_size\n",
    "    if len(time_index) % chunk_size != 0:\n",
    "        num_chunks += 1\n",
    "\n",
    "    all_anomalies = []\n",
    "    all_scores = []\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, len(time_index))\n",
    "        \n",
    "        # Extract chunk\n",
    "        chunk_time_index = time_index[start_idx:end_idx]\n",
    "        chunk_scores = scores[start_idx:end_idx]\n",
    "        \n",
    "        # Compute mean and std deviation for the chunk\n",
    "        mean_score = np.mean(chunk_scores)\n",
    "        std_dev_score = np.std(chunk_scores)\n",
    "        \n",
    "        # Compute z-scores and detect anomalies\n",
    "        z_scores = (chunk_scores - mean_score) / std_dev_score\n",
    "        anomalies = z_scores > threshold\n",
    "        \n",
    "        # Append results\n",
    "        all_anomalies.append(anomalies)\n",
    "        all_scores.append(chunk_scores)\n",
    "        \n",
    "    # Combine results\n",
    "    combined_anomalies = np.concatenate(all_anomalies)\n",
    "    combined_scores = np.concatenate(all_scores)\n",
    "    \n",
    "    return combined_anomalies, combined_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba14d0-f25b-4149-ada8-2fc1fb63a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate anomaly scores by chunk\n",
    "chunk_anomalies, chunk_scores = calculate_anomaly_scores_by_chunk(time_index, scores, chunk_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43720e2-4d10-4031-ad24-d6fca186ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the anomaly scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time_index, chunk_scores, label=\"Anomaly Scores\")\n",
    "plt.scatter(time_index[chunk_anomalies], chunk_scores[chunk_anomalies], color='red', label=\"Detected Anomalies\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Anomaly Scores with Detected Anomalies (by Chunks)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64110be-d53e-44f1-a79a-b431558fc115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the indices of detected anomalies\n",
    "print(\"Anomaly indices:\", np.where(chunk_anomalies)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aa7ab7-e555-43fc-a01d-afb14dbabb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data: Replace these with your actual data\n",
    "time_index = np.arange(len(scores))  # Assuming time index is sequential\n",
    "normal_data = np.random.normal(0, 1, len(scores))  # Replace with actual normal ECG data\n",
    "anomalous_data = np.random.normal(0, 1, len(scores))  # Replace with actual anomalous ECG data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc67ee79-a7de-42bb-ab1f-2803f04fad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot ECG data with anomalies\n",
    "def plot_ecg_with_anomalies(time_index, normal_data, anomalous_data, scores, anomalies, threshold=3):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Plot normal ECG data\n",
    "    plt.plot(time_index, normal_data, label=\"Normal ECG Data\", color='blue', alpha=0.5)\n",
    "    \n",
    "    # Plot anomalous ECG data\n",
    "    plt.plot(time_index, anomalous_data, label=\"Anomalous ECG Data\", color='orange', alpha=0.5)\n",
    "    \n",
    "    # Plot anomaly scores\n",
    "    plt.plot(time_index, scores, label=\"Anomaly Scores\", color='green', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Highlight detected anomalies\n",
    "    plt.scatter(time_index[anomalies], scores[anomalies], color='red', label=\"Detected Anomalies\", marker='x')\n",
    "    \n",
    "    plt.axhline(y=threshold, color='red', linestyle='--', label=\"Threshold\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('ECG Data with Detected Anomalies')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e06c2-1209-4b0f-967a-58453c3d1ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "plot_ecg_with_anomalies(time_index, normal_data, anomalous_data, scores, chunk_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a933d265-701e-42f7-aaf2-378f38b2ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data: Replace these with your actual data\n",
    "time_index = np.arange(len(scores))  # Assuming time index is sequential\n",
    "ecg_data = np.random.normal(0, 1, len(scores))  # Replace with your actual ECG data\n",
    "chunk_start = 100  # Define start index for the chunk\n",
    "chunk_end = 200  # Define end index for the chunk\n",
    "\n",
    "# Define the threshold for anomalies (e.g., z-score > 3)\n",
    "threshold = 3\n",
    "\n",
    "# Compute z-scores (assuming you already have scores and anomalies)\n",
    "mean_score = np.mean(scores)\n",
    "std_dev_score = np.std(scores)\n",
    "z_scores = (scores - mean_score) / std_dev_score\n",
    "anomalies = z_scores > threshold\n",
    "\n",
    "# Extract the chunk of data\n",
    "chunk_time_index = time_index[chunk_start:chunk_end]\n",
    "chunk_ecg_data = ecg_data[chunk_start:chunk_end]\n",
    "chunk_scores = scores[chunk_start:chunk_end]\n",
    "chunk_anomalies = anomalies[chunk_start:chunk_end]\n",
    "\n",
    "# Function to plot a chunk of ECG data with normal and anomalous segments\n",
    "def plot_ecg_chunk_with_normal_and_anomalous_segments(time_index, ecg_data, scores, anomalies, threshold):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    \n",
    "    # Plot segments\n",
    "    current_color = 'green'\n",
    "    for i in range(len(time_index) - 1):\n",
    "        # Switch color when an anomaly is detected\n",
    "        if anomalies[i]:\n",
    "            plt.plot(time_index[i:i+2], ecg_data[i:i+2], color='red', alpha=0.8)\n",
    "        else:\n",
    "            plt.plot(time_index[i:i+2], ecg_data[i:i+2], color=current_color, alpha=0.8)\n",
    "    \n",
    "    # Plot anomaly scores\n",
    "    plt.plot(time_index, scores, label=\"Anomaly Scores\", color='blue', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Highlight detected anomalies\n",
    "    plt.scatter(time_index[anomalies], scores[anomalies], color='red', label=\"Detected Anomalies\", marker='x')\n",
    "    \n",
    "    plt.axhline(y=threshold, color='red', linestyle='--', label=\"Threshold\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('ECG Value')\n",
    "    plt.title('ECG Chunk with Normal and Anomalous Segments')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the chunk\n",
    "plot_ecg_chunk_with_normal_and_anomalous_segments(chunk_time_index, chunk_ecg_data, chunk_scores, chunk_anomalies, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b2707-01c7-4d57-8dd5-3381118ca02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time Index Chunk:\", chunk_time_index)\n",
    "print(\"ECG Data Chunk:\", chunk_ecg_data)\n",
    "print(\"Anomalies Chunk:\", chunk_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1659fd6-d5f2-4ee3-920e-14f49607716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.any(chunk_anomalies):\n",
    "    print(\"Anomalies detected in chunk.\")\n",
    "else:\n",
    "    print(\"No anomalies detected in chunk.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dddf8c-79bd-4b77-a5e6-b60b5f386dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "for i in range(len(chunk_time_index) - 1):\n",
    "    plt.plot(chunk_time_index[i:i+2], chunk_ecg_data[i:i+2], color='red' if chunk_anomalies[i] else 'green', alpha=0.8)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('ECG Value')\n",
    "plt.title('ECG Chunk with Normal and Anomalous Segments')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
